GROUP ID
---------
18


Dataset choice

Which dataset from the curated list did you choose?  If you plan to use more than one, select the
primary one.  If you selected a different dataset, use the open option and list it down, and you'll
 need to discuss with our instruction staff.

It's highly encouraged that you select one of the curated datasets as your primary project
data source.

Details of the curated datasets are on the CS3244 Project Datasets document: http://www.comp.nus.edu.sg/~cs3244/AY2122S1/datasets.html

---------------------------------

Sarcasm Detection

---------------------------------


Dataset description

Write in your own words (1-2 sentences) what this dataset describes.

Rubric:

a) Does your description specify features and instances?

b) Do you describe the source and reliability of the data?

---------------------------------

The dataset contains comments from a commentary website, Reddit, where comments with and without the \s tag were scraped. The dataset contains features such as the contents of the comment, author, subreddit, score, upvotes, downvotes and date, as well as parent comment(if any). The training set has over 1 million instances.

The data was gathered by researchers from the Computer Science Department of Princeton University for their paper “A Large Self-Annotated Corpus for Sarcasm”. The dataset also has a usability score of 10/10 on Kaggle which means it is curated, hence it is likely to be reliable.


---------------------------------

Project Title

Please be concise, relevant and descriptive.

---------------------------------

Sentiment Analysis: Sarcasm detection of Reddit comments

---------------------------------

Motivation

Explain why this project is interesting and important.

Rubric:

a) Does your motivation clearly describe a problem?

b) Does it justify the problem’s significance? What are the benefits of addressing this problem? Who benefits from solving it?

---------------------------------

Online comments can be a mix of sarcastic and non-sarcastic comments. Users, who are not be able to recognize that a particular comment is indeed sarcastic and not meant to be taken seriously, may have the tendency to get offended and reply in a negative manner. Sentiment analysis of texts will allow big amounts of text data to be categorized appropriately, in this case, whether or not it is sarcastic. Automatic sarcasm detection of such texts in a website may prevent users from misinterpreting and taking sarcastic comments too seriously, creating a safer commentary space by reducing the chance of negative responses. This benefits not only the users but also the admins of such commentary websites and more heavily moderated forums, as it has the potential to reduce conflict and inappropriate comments.

---------------------------------

Statement of the Problem/Task

A statement of the problem, issue, or task that you’re interested in studying. In particular,
try to formulate the key questions (2 to 4 questions is probably a good number) that your group will answer in the project.

As this proposal document is self-contained, you should restate your project topic and domain.

The information you provide here will also help to assign appropriate reviewers.

Rubric:

a) Does the proposal outline a problem statement, issue, or task that the group is interested
in studying?

b) Does it formulate a few (2 to 4) questions that the group proposes to address?

---------------------------------

This project aims to find out whether a text comment made by a Reddit user belongs to a specific sentiment - sarcasm. Sarcasm detection can help in better understanding the comment and a suitable response to said comment.  Generally, we wish to find out which features are correlated to sarcasm. We would like to attempt to find the answer to the questions below:

1. Natural language processing - which words/phrases are more reliable in indicating sarcasm? Are there other linguistic features that predict sarcasm well? E.g. capitalized words, ellipses 

2. Feature engineering - which features can provide us with the most insight when predicting sarcasm? For instance, do users tend to make more sarcastic comments depending on the subreddit they are on? Do sarcastic comments score better/worse and receive more/fewer upvotes?

3. Model selection - We will be exploring 3 models - SVM, logistic regression and transformers, in order to find out which of these can most accurately predict sarcasm in Reddit text comments.



---------------------------------

General Approach

A high-level description of the general approach you’ll use to address the questions.
Survey on the current progress on the problem/task. Sketch out what evidence you are planning
to gather (e.g., how you can answer the questions through experiments on the data).

Rubric:

a) Does the proposal contain a high-level draft description of the general approach proposed to address the questions?

b) Does it include preliminary plans for evaluation, data gathering? I.e., how the team plans to answer the questions through experiments on data.

---------------------------------

There has been existing research into this topic, but it appears that sarcasm is one of the tougher sentiments to identify as it is harder to define well without listening to the person's tone (unlike hate comments that generally have a negative sentiment).

To identify common sarcastic words, we can identify key phrases that contain capitalized words or letters in the middle of the text generally signifying that it is sarcastic, due to the emphasis/stress placed. There are also certain common words used in the majority of sarcastic comments like “always”, “obviously”, and ”surprise”, which can help us better classify the comments.

To understand which sub-reddit tends to be filled with more sarcastic comments, we take the training data and group all the comments by their respective sub-reddits. We then calculate the number of sarcastic comments each sub-reddit contains using the target label. We compare this with test data and see if the trend follows for the sub-reddits. 

To understand up-votes/down-votes and score trends for sarcastic comments, we compare sarcastic comments against normal comments and calculate the average ratio of up-vote/down-votes as well as average scores.

To use the text comments in the ML models, we can do feature engineering and derive several features from the comments such as length, presence of words commonly associated with sarcasm, up-votes/down-votes etc. These new features can then be used in SVM and logistic regression, along with the other features.

---------------------------------



Evaluation

Include how you will evaluate your project.
Propose what your team thinks is a satisfactory project outcome (C grade) and an excellent project
outcome (A grade).

Rubric:

a) Does the proposal contain a high-level draft description of the general approach proposed
to address the questions?

b) Does it include preliminary plans for evaluation, data gathering?
I.e., how the team plans to answer the questions through experiments on data.

---------------------------------

We can find out which words/phrases/linguistic features are better indications of sarcasm by analysing which of them appear most frequently in sarcastic comments.

To find out which features are most relevant in predicting sarcastic comments we can simply compare the performances and model accuracy when using different feature subsets, and rank them.

We will compare the accuracy of each of our 3 machine learning models (SVM, logistic regression, transformer) based on their performance on the provided test dataset (i.e. hold-out validation), and evaluate our project based on the model with the highest accuracy of the 3.
Satisfactory outcome: >50% accuracy for some model
Excellent outcome: >70% accuracy for some model


---------------------------------


Additional Resources

A list of resources you have/need to conduct the project.
This includes additional reading, software, datasets, code (Github link), etc.,
beyond your chosen dataset.  Are these resources public? How are you planning to get these resources?

Rubric

a) Does the proposal give a short list of resources the team plans to use to execute the project (inclusive of readings, software, datasets, etc.)?

b) Does the team describe any strategy for getting the resources?

---------------------------------

Paper describing the dataset:https://arxiv.org/abs/1704.05579

Overview of sarcasm detection using NLP: https://towardsdatascience.com/sarcasm-detection-with-nlp-cbff1723f69a

Guide to text classification: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/ 

Google guide to text classification: https://developers.google.com/machine-learning/guides/text-classification 

Kaggle guide to text classification using SpaCy: https://www.kaggle.com/matleonard/text-classification 

Guide to NLP with SpaCy: https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/ 

Text classification with BERT Transformer model: https://www.analyticsvidhya.com/blog/2021/06/why-and-how-to-use-bert-for-nlp-text-classification/ 
https://www.tensorflow.org/text/tutorials/classify_text_with_bert 
 
Guide to feature selection:
https://www.datacamp.com/community/tutorials/feature-selection-python
 
The reading materials are free and public.
Strategy: Upon encountering anything we are unsure of, we google and try to find online resources that answer our query. If we are unable to find a satisfying answer, we will ask our TAs and professors for assistance.


---------------------------------

Schedule / Role Assignment

A schedule of work indicating the dates by which you plan to complete components of the project.
Make sure the schedule is plausible.

You may find that a table format with the remaining weeks of the course helpful to describe this goal.

Rubric:

a) A schedule indicating dates by which the team plans to complete the project components?

b) An assignment of the team members to the deliverables (inclusive of peer reviewing duties)?

c) Is the schedule feasible given the timeline, expertise and load of the team members?

d) For projects with large data sources, does the team propose a way to scope the data or
problem accordingly to make it feasible?

---------------------------------

Week 4-5:
Explore datasets (Everyone)
Discuss and select project idea and dataset (Everyone)
Submit project proposal (12 Sep) (Everyone)

Week 6:
Exploratory data analysis and visualisation to understand data (Haritha, Araavind, Amas)
Research on relevant ML models and techniques, E.g. NLP methods for analysing comments (Joon Jie, Glendon)
Proposal peer critique (19 Sep) (Everyone)

Recess week:
Develop detailed methodology, eg. how to make use of text with the other features (Haritha, Araavind, Amas)
Feature engineering: decompose text comments into several features (Joon Jie)
Feature selection: choose which features to use (Glendon)
 
Week 7:
Logistic Regression

Week 8:
Logistic Regression evaluation

Week 9:
SVM

Week 10:
SVM with kernel evaluation

Week 11:
Transformer model

Week 12:
Transformer model evaluation

For weeks 7 to 12:
For all three models, it will follow the outline below:

Data cleaning/preprocessing, setting up/preparing the model (Haritha)
Training the model on the training data (Joon Jie)
Testing and evaluating the model (Glendon)
Parameter tuning (Amas)
Stats collection (Araavind)

These roles are assigned to each person tentatively, if possible, the roles will be rotated among individuals of the group for each separate model. This is to allow all members to be exposed to tackle different portions of the model development process. 

Week 13:
Leftover tasks from previous weeks (Everyone)
Overall evaluation of the models (Everyone)
Explore ensemble models if time permits (Everyone)

Reading week:
Start planning project presentation (Everyone)
Create presentation slides(Everyone)

Exam week:
Finalise project (Everyone)
Plan and record project presentation (Everyone)
Submit project presentation (28 Nov) (Everyone)

---------------------------------